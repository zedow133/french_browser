import ffmpeg
import os
import shutil
import sys
from pathlib import Path
from transnetv2 import TransNetV2
import tempfile
from analysis_keyframes_clip import single_embedding
from sklearn.cluster import AgglomerativeClustering


def get_video_fps(video_path):
    probe = ffmpeg.probe(video_path, select_streams='v:0')
    fps_str = probe['streams'][0]['r_frame_rate']
    if '/' in fps_str:
        num, den = fps_str.split('/')
        fps = float(num) / float(den)
    else:
        fps = float(fps_str)
    return fps

def extract_keyframes(video_path, start_frame, end_frame, fps, keyframes_dir, shot_id, number_extracted, device, clip_model, preprocess):
    # Extraction of number_extracted (default = 10) keyframes between start_frame and end_frame, saved in a temporary directory
    # A safeguard zone of 10% from the beginning and 10% to the ending is used to ensure no "transition" keyframes
    safe_start_frame = start_frame + 0.1*(end_frame - start_frame)
    safe_end_frame = end_frame - 0.1*(end_frame - start_frame)

    timestamps = [(safe_start_frame + (((i + 1)*(safe_end_frame - safe_start_frame)) // (number_extracted + 1))) / fps for i in range(number_extracted)]

    temp_dir = tempfile.mkdtemp()
    temp_image_paths = []
    
    for i, timestamp in enumerate(timestamps):
        temp_output_path = os.path.join(temp_dir, f"keyframe_{i:03d}.jpg")
        (
            ffmpeg
            .input(video_path, ss=timestamp)
            .output(temp_output_path, vframes=1, **{'q:v': 2})
            .overwrite_output()
            .run(quiet=True)
        )
        temp_image_paths.append(temp_output_path)
    
    # Two categories of embeddings are computed
    # → pure_image_embeddings are embeddings directly generated by CLIP, and will be saved in the database
    # → image_clustering_embeddings are pure_image_embeddings which are normalised and transformed to numpy arrays to allow keyframe clustering
    pure_image_embeddings = []
    image_clustering_embeddings = []
    

    for image_path in temp_image_paths:
        pure_image_embedding = single_embedding(image_path, preprocess, device, clip_model)
        pure_image_embeddings.append(pure_image_embedding)

        image_clustering_embedding = pure_image_embedding / pure_image_embedding.norm(dim=-1, keepdim=True)
        image_clustering_embedding = image_clustering_embedding.detach().numpy()
        image_clustering_embeddings.append(image_clustering_embedding[0])

    # Clustering of the extracted keyframes to keep only keyframes with significant (> 50%) difference
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=0.5,
        linkage='average'
    )
    
    labels = clustering.fit_predict(image_clustering_embeddings)

    # Construct the final lists of embeddings and image names, also save in the keyframes folder of the video (keyframes_dir)
    final_embeddings = []
    final_image_names = []

    unique_labels = set(labels)
    for number_image, unique_label in enumerate(unique_labels):
        i = labels.tolist().index(unique_label)

        output_base_path = os.path.join(keyframes_dir, f"{shot_id}")
        image_path = f'{output_base_path}_{number_image}.jpg'
        shutil.copy2(temp_image_paths[i], image_path)
        final_image_names.append(f"{shot_id}_{number_image}")

        final_embeddings.append(pure_image_embeddings[i])

    return final_image_names, final_embeddings

def extract_video_clip(video_path, start_frame, end_frame, fps, output_path):
    # Extraction of a video clip beween a given start and end frame
    start_timestamp = start_frame / fps
    duration_frames = end_frame - start_frame + 1
    duration_seconds = duration_frames / fps
    
    (
        ffmpeg
        .input(video_path, ss=start_timestamp, t=duration_seconds)
        .output(
            output_path,
            vcodec='libx264',
            acodec='aac',
            preset='medium',
            crf=23
        )
        .overwrite_output()
        .run(quiet=True)
    )

def detect_shots_with_transnet(video_path, model):
    # Detect shot boundaries using TransnetV2
    video_frames, single_frame_predictions, all_frame_predictions = model.predict_video(video_path)
    return model.predictions_to_scenes(single_frame_predictions).tolist()